<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# C04-决策树
## 4.1 基本流程
* 决策树是基于树结构来进行决策的
* 一般的，一棵决策树包含一个根节点、若干个内部节点和若干个叶节点，叶节点对应于决策结果，其他每个节点对应于一个属性测试，每个节点包含的样本集合根据属性测试的结果被划分到子节点中，根节点包含样本全集，从根节点到每个叶节点的路径对应了一个判定测试序列
* 决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。

![](./picture/C04/DecisionTreeAlgorithm.png)
## 4.2 划分选择
我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的纯度(purity)越来越高
### 4.2.1 信息增益
* 信息熵(information entropy)是度量样本集合纯度最常用的一种指标，假定当前样本集合*D*中第*k*类样本所占比例为为\\(p _k(k=1,2,\cdots,|y|)\\)，则*D*的信息熵定义为
$$
Ent(D)=-\sum _{k=1}^{|y|}p _klog _2p _k 
$$
值越小，*D*的纯度越高
* 假定离散属性*a*有*V*个可能的取值{\\(a^1,a^2,\cdots,a^V\\)}，若使用*a*来对样本*D*进行划分，则会产生V个分支节点，其中第*v*个分支节点包含了*D*中所有在属性*a*上取值为\\(a^v\\)的样本，记为\\(D^v\\)，可以计算出\\(D^v\\)的信息熵，从而得到用属性*a*对样本集*D*进行划分所获得的“信息增益”(information gain)：
$$
Gain(D, a)=Ent(D)-\sum _{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)
$$
一般而言，信息增益越大则意味着使用属性*a*来进行划分所获得的“纯度提升”越大，即选择属性\\(a _*=argmax _{a\in A}Gain(D, a)\\)
### 4.2.2 增益率
* 信息增益准则对可取值数目较多的属性有所偏好，通过使用增益率(gain ratio)可以减少这种偏好所带来的影响，增益率定义为
$$
Gain_ratio(D, a)=\frac{Gain(D,a)}{IV(a)}
$$
其中
$$
IV(a)=-\sum _{v=1}^V\frac{|D^v|}{|D|}log _2\frac{|D^v|}{|D|}
$$
称为属性*a*的固有值(intrinsic value)
* 增益率准则对可取值数目较少的属性有所偏好，所以可以使用一种启发式方法：先从候选划分属性中找到信息增益高于平均水平的属性，再从中选择增益率高的
### 4.2.3 基尼指数
* CART决策树使用基尼指数(Gini index)来选择划分属性。CART(Classification and Regression Tree)是一种著名的决策树算法，分类和回归都可以用
* 数据集*D*的纯度可用基尼值来度量：
$$
\begin{equation}
\begin{aligned}
Gini(D)&=\sum _{k=1}^{|y|}\sum _{k'\ne k}p _kp' _k \\\\
&=1-\sum _{k=1}^{|y|}p^2 _k
\end{aligned}
\end{equation}
$$
* 直观来说，\\(Gini(D)\\)反映了从数据集*D*中随机抽取两个样本，其类别标记不一致的概率，因此基尼值越小，则数据集*D*的纯度越高。属性*a*的基尼指数定义为
$$
GiniIndex(D,a)=\sum _{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)
$$
于是我们在候选属性集合*A*中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即\\(a _*=argmin _{a\in A}GiniIndex{D,a}\\)
## 4.3 剪枝处理
* 剪枝(pruning)是决策树学习算法对付过拟合的主要手段，基本策略有“预剪枝”(prepruning)和“后剪枝”(postpruning)。预剪枝是在决策树生成过程中，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换成叶节点能带来决策树泛化能力的提升，则将该子树替换为叶节点
* 判断决策树泛化能力是否提升，可以使用留出法，即预留部分数据作为验证集以进行性能评估
### 4.3.1 预剪枝
![](./picture/C04/prepruning.png)
* 预剪枝使得决策树的很多分支都没有展开，不仅降低了过拟合的风险，还显著减少了开销，但是提高了欠拟合的风险
### 4.3.2 后剪枝
![](./picture/C04/postpruning.png)
* 后剪枝先从训练集生成一棵完整的决策树，然后从下往上对非叶节点考察是否剪枝。一般情况下，欠拟合风险很小，泛化性能有所强化，但是开销时间要大很多
## 4.4 连续与缺失值
### 4.4.1 连续值处理
* 连续属性的划分最简单的策略是采用二分法(bi-partiton)
* 给定样本集*D*和连续属性*a*，假定*a*在*D*上出现了n个不同的取值，将这些值从小到大排序，记为{\\(a^1,a^2, \cdots,a^n\\)}。基于划分点*t*可将*D*分为子集\\(D^- _t和D^+ _t\\)，分别表示那些在属性*a*上取值不大于*t*的样本以及那些大于*t*的样本，然后对连续属性*a*，可以考察包含n-1个元素的候选集合
$$
T _a=\lbrace\frac{a^i+a^{i+1}}{2}|1\le i\le n-1 \rbrace
$$
然后就可以像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分
* 需要注意的是，与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性
### 4.4.2 缺失值处理
* 需要解决两个问题（1）如何在属性值缺失的情况下进行划分属性选择（2）给定划分属性，若样本在该属性的值缺失，如何对样本进行划分
* 给定训练集*D*和属性*a*，令\\(\tilde{D}\\)表示*D*中在属性*a*上没有缺失值的样本子集。对于问题(1)，使用\\(\tilde{D}\\)来划分即可。假定属性*a*有*V*个可取值{\\(a^1,a^2,\cdots,a^V\\)}，令\\(\tilde{D}^v\\)表示\\(\tilde{D}\\)在属性a上取值为\\(a^v\\)，\\(\tilde{D} _k\\)表示\\(\tilde{D}\\)中属于第*k*类的样本子集，假定为每个样本\\(\vec{x}\\)确定一个权重\\(\omega _{\vec{x}}\\)，并定义
$$
\begin{equation}
\begin{aligned}
\rho &=\frac{\sum _{\vec{x}\in \tilde{D}}\omega _{\vec{x}}}{\sum _{\vec{x}\in D}\omega _{\vec{x}}} \\\\
\tilde{p} _k &=\frac{\sum _{\vec{x}\in \tilde{D} _k}\omega _{\vec{x}}}{\sum _{\vec{x}\in D}\omega _{\vec{x}}} \qquad (1\le k\le |y|) \\\\
\tilde{r} _k &=\frac{\sum _{\vec{x}\in \tilde{D}^v}\omega _{\vec{x}}}{\sum _{\vec{x}\in D}\omega _{\vec{x}}} \qquad (1\le v\le V)
\end{aligned}
\end{equation}
$$
信息增益可以推广位：
$$
\begin{equation}
\begin{aligned}
Gain(D, a) &=\rho\times Gain(\tilde{D}, a)\\\\
&=\rho\times\big(Ent(\tilde{D})-\sum _{v=1}^V\tilde{r} _vEnt(\tilde{D}^v)\big) \\\\
Ent(\tilde{D}) &=-\sum _{k=1}^{|y|}\tilde{p} _klog _2\tilde{p} _k
\end{aligned}
\end{equation}
$$
* 对于问题(2)，若样本在某种属性上值缺失，则将该样本同时划入所有子节点，并且将样本权值修改为\\(\tilde{r} _v\times\omega _{\vec{x}}\\)，直观上看，就是将该样本以不同的概率划分进不同的子节点中去
## 4.5 多变量决策树
* 若我们把每个属性视为坐标空间的坐标轴，则*d*个描述的样本就对应了*d*维空间的一个数据点，对样本分类则意味着在这个坐标空间中寻找分类边界。决策树所形成的边界有一个明显的特点：分类边界与若干个坐标轴平行
* 多变量决策树(multivariate decision tree)每个节点不再是单独的属性划分，而是一个融合了多个属性的线性分类器
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# C10-降维与度量学习
## 10.1 k近邻学习
* k近邻(k-Nearest Neighbor,kNN)学习是一种常用的监督学习方法，给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这*k*个邻居的信息来进行预测。通常在分类任务中可以使用投票法，在回归任务中可以使用平均法，当然也可以基于距离远近使用加权平均或加权投票。
## 10.2 低维嵌入
* 在高维情形下出现样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为维数灾难(curse of dimensionality)，缓解维数灾难的一个重要途径是降维(dimension reduction)，即通过某种数学变换将原始高维属性空间转变为一个低维子空间，在这个子空间样本密度大幅度提高，距离计算也变得容易.
* 如果要求原始空间中样本之间的距离在低维空间中得到保持，即得到多维缩放(Multiple Dimensional Scaling,MDS)算法：假定*m*个样本在原始空间的距离矩阵为\\(\matrix{D}\in\mathbb{R}^{m\times m}\\)，其第*i*行第*j*列的元素为样本\\(\vec{x} _i和\vec{x} _j\\)的距离，我们的目标是获得样本在*d'*维空间的表示\\(\matrix{Z}\in\mathbb{R}^{R'\times m},d'\le d\\)，且任意两个样本在*d'*维空间中的欧式距离等于原始空间中的距离，即\\(|\vec{z} _i-\vec{z} _j|=dist _{ij}\\)
* 令\\(\matrix{B}=\matrix{Z}^T\matrix{Z}\in\mathbb{R}^{m\times m}\\)，其中\\(\matrix{B}\\)为降维后样本的内积矩阵，\\(b _{ij}=\vec{z} _i^T\vec{z} _j\\)，有
\begin{aligned}
dist _{ij}^2&=||\vec{z} _i||^2+||\vec{z} _j||^2-2\vec{z} _i^T\vec{z} _j\\\\
&=b _{ii}+b _{jj}-2b _{ij}
\end{aligned}
令降维后的样本\\(\matrix{Z}\\)被中心化，即\\(\sum _{i=1}^m\vec{z} _i=\vec{0}\\)，令
\begin{aligned}
dist _{i\cdot}^2&=\frac{1}{m}\sum _{j=1}^mdist _{ij}^2=\frac{1}{m}tr(\matrix{B})+b _{ii}\\\\
dist _{\cdot j}^2&=\frac{1}{m}\sum _{i=1}^mdist _{ij}^2=\frac{1}{m}tr(\matrix{B})+b _{jj}\\\\
dist _{\cdot\cdot}^2&=\frac{1}{m^2}\sum _{j=1}^m\sum _{i=1}^mdist _{ij}^2=\frac{2}{m}tr(\matrix{B})
\end{aligned}
那么就可以通过下式来求取内积矩阵\\(\matrix{B}\\)
\begin{aligned}
b _{ij}=-\frac{1}{2}(dist _{ij}^2-dist _{i\cdot}^2-dist _{\cdot j}^2+dist _{\cdot\cdot}^2)
\end{aligned}
* 对矩阵\\(\matrix{B}\\)做特征值分解(eigenvalue decomposition)，\\(\matrix{B}=\matrix{V}\matrix{\Lambda}\matrix{V}^T\\)，其中\\(\matrix{\Lambda}=diag(\lambda _1,\lambda _2,\cdots,\lambda _d)\\)为特征值构成的对角矩阵，\\(\lambda _1\ge\lambda _2\ge\cdots\lambda _d\\)，由于在现实中的有效降维往往仅需要降维后的距离与原始空间中的距离尽可能接近，而不必严格相等，此时可取\\(d'\ll d\\)个最大特征值构成对角矩阵\\(\tilde{\matrix{\Lambda}}=diag(\lambda _1,\lambda _2,\cdots,\lambda _{d'})\\)，令\\(\tilde{\matrix{V}}\\)表示对应的特征向量矩阵，则\\(\matrix{Z}\\)可表达为$$\matrix{Z}=\tilde{\matrix{\Lambda}}^{1/2}\tilde{\matrix{V}}^T\in\mathbb{R}^{d'\times m}$$
* 对降维效果的评估，通常是比较降维前后学习器的性能，若性能有所提高则认为降维起到了作用，若将维数降至二维或三维，则可通过可视化技术来直观判断
## 10.3 主成分分析(Principal Component Analysis, PCA)
* 对于正交属性空间的样本点，用一个超平面对所有的样本进行恰当的表达，一般考虑（1）最近重构性：样本点到这个超平面的距离都足够近；（2）最大可分性：样本点在这个超平面上的投影能尽可能分开
* 对于PCA，一般需要对样本的平均值和方差进行规范化
\begin{aligned}
\vec{x} _i&\gets\vec{x} _i-\frac{1}{m}\sum _{i=1}^m\vec{x} _i\\\\
x _{ij}&\gets\frac{x _{ij}}{\sqrt{\frac{1}{m}\sum _{i=1}^mx _{ij}^2}}
\end{aligned}
![](./picture/C10/PCA.png)
* 降维后的低维空间维数*d'* 通常由用户事先指定，或通过在*d'* 值不同的低维空间中对*k*近邻分类器或其他开销较小的学习器进行交叉验证来选取较好的*d'* 值。或者可以从重构的角度设置一个重构阈值，例如\\(t=95\\%\\)，然后根据下式来选取最小的*d'* 值
$$
\frac{\sum _{i=1}^{d'}\lambda _i}{\sum _{i=1}^d\lambda _i}\ge t
$$
* 降维导致部分信息丢失，一方面，舍弃这部分信息可以使得样本的采样密度增大，这是降维的主要动机，另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，舍弃可以起到一定的去噪作用
## 10.4 核化线性降维
* 非线性降维的一种方法，是基于核技巧对线性降维方法进行核化(kernelized)
## 10.5 流形学习
## 10.6 度量学习
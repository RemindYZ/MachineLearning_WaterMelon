<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# C03-线性模型
## 3.1 基本形式
给定由*d*个属性描述的示例\\(\vec{x}=(x_1,x_2,...,x_d)\\)，其中\\(x_i\\)是\\(\vec{x}\\)在第*i*个属性上的取值，线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数，即$$f(\vec{x}=\omega_1x_1+\omega_2x_2+...+\omega_dx_d+b)$$用向量形式写成$$f(\vec{x})=\vec{\omega}^T\vec{x}+b$$其中$$\vec{\omega}=(\omega_1,\omega_2,...,\omega_d)$$
## 3.2 线性回归
* 多元线性回归(multivariate linear regression)$$f(\vec{x} _i)=\vec{\omega}^T\vec{x} _i+b _i$$使得\\(f(\vec{x} _i)\simeq y _i\\)
* 用最小二乘法对\\(\vec{\omega}\\)和*b*进行估计，令
$$
\vec{X}=\begin{pmatrix}
        x_{11} & x_{12} & \cdots & x_{1d} & 1\\\\
        x_{21} & x_{22} & \cdots & x_{2d} & 1\\\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\\\
        x_{m1} & x_{m2} & \cdots & x_{md} & 1
        \end{pmatrix}
=
\begin{pmatrix}
\vec{x_1}^T & 1\\\\
\vec{x_2}^T & 1\\\\
\vdots & \vdots\\\\
\vec{x_m}^T & 1\\\\
\end{pmatrix}
$$
将标记也写成向量形式\\(\vec{y}=(y_1,y_2,y_3...,y_m)\\)，有$$\widehat{\vec{\omega}}^*=argmin_{\vec{\omega}}(\vec{y}-\vec{X}\widehat{\vec{\omega}})^T(\vec{y}-\vec{X}\widehat{\vec{\omega}})$$
* 当\\(\vec{X}^T\vec{X}\\)为满秩矩阵(full-rank matrix)或者正定矩阵(positive definite matrix)时，有$$\widehat{\vec{\omega}}^*=(\vec{X}^T\vec{X})^{-1}\vec{X}^T\vec{y}$$
* 如果\\(\vec{X}^T\vec{X}\\)不是满秩矩阵，一般会引入正则化(regularization)
* 更一般地，考虑单调可微函数\\(g(·)\\)，令$$y=g^{-1}(\vec{\omega}^T\vec{x}+b)$$
## 3.3 对数几率回归
* 对于分类任务，在上述的广义模型中，，只需要找一个单调可微函数将分类任务的真实标记\\(y\\)与线性回归模型的预测值联系起来，一般是使用sigmoid function或者logistic function：$$y=\frac{1}{1+e^{-z}}$$即$$y=\frac{1}{1+e^{-(\vec{\omega}^T\vec{x}+b)}}$$可以变化为$$ln{\frac{y}{1-y}}=\vec{\omega}^T\vec{x}+b$$\\(y\\)可以视为样本预测结果为正例的可能性，\\(1-y\\)是反例的可能性
* 将\\(y\\)视为类后验概率估计\\(p(y=1|\vec{x})\\)，有
$$ln{\frac{p(y=1|\vec{x})}{p(y=0|\vec{x})}}=\vec{\omega}^T\vec{x}+b$$
显然有
$$p(y=1|\vec{x})=\frac{e^{\vec{\omega}^T\vec{x}+b}}{1+e^{\vec{\omega}^T\vec{x}+b}}$$
$$p(y=0|\vec{x})=\frac{1}{1+e^{\vec{\omega}^T\vec{x}+b}}$$
通过极大似然法(maximum likelihood method)来估计\\(\vec{\omega}^T\\)和\\(b\\)。
给定数据集\\(\lbrace(\vec{x} _i,y _i)\rbrace^m _{i=1}\\)，对数几率回归模型最大化“对数似然”(log-likelihood)
$$
l(\vec{\omega},b)=\sum _{i=1}^mlnp(y _i|\vec{x} _i;\vec{\omega},b)
$$
令\\(\vec{\beta}=(\vec{\omega};b)\\)，\\(\hat{\vec{x}}=(\vec{x};1)\\)，\\(p _1(\hat{\vec{x}};\vec{\beta})=p(y=1|\hat{\vec{x}};\vec{\beta})\\)，\\(p _0(\hat{\vec{x}};\vec{\beta})=p(y=0|\hat{\vec{x}};\vec{\beta})=1-p _1(\hat{\vec{x}};\vec{\beta})\\)，有
$$p(y _i|\vec{x} _i;\vec{\omega},b)=y _ip _1(\hat{\vec{x} _i};\vec{\beta})+(1-y _i)p _0(\hat{\vec{x}} _i;\vec{\beta})$$
于是，最大化对数似然等价于最小化下式（注意到\\(y _i\in \lbrace 0,1\rbrace\\)）:
$$l(\vec{\beta})=\sum _{i=1}^m(-y _i\vec{\beta}^T\hat{\vec{x}} _i+ln(1+e^{\vec{\beta}\hat{\vec{x}} _i}))$$
这是关于\\(\vec{\beta}\\)的高阶可导连续凸函数，可使用经典数值优化算法如梯度下降法(gradient descent method)、牛顿法(Newton method)等都可以求最优解，得到
$$
\vec{\beta}^*=argmin _{\vec{\beta}}l(\vec{\beta})
$$
* 以牛顿法为例，迭代公式如下：$$\vec{\beta}^{t+1}=\vec{\beta}^t-(\frac{\partial^2l(\vec{\beta})}{\partial\vec{\beta}\partial\vec{\beta}^T})^{-1}\frac{\partial l(\vec{\beta})}{\partial\vec{\beta}}$$
其中
$$
\frac{\partial l(\vec{\beta})}{\partial \vec{\beta}}=-\sum _{i=1}^m\hat{\vec{x}} _i(y _i-p _1(\hat{\vec{x}} _i;\vec{\beta}))
$$
$$
\frac{\partial^2l(\vec{\beta})}{\partial\vec{\beta}\partial\vec{\beta}^T}=\sum _{i=1}^m\hat{\vec{x}} _i\hat{\vec{x}} _i^Tp _1(\hat{\vec{x}} _i;\vec{\beta})(1-p _1(\hat{\vec{x}} _i;\vec{\beta}))
$$
## 3.4 线性判别分析(Linear Discriminant Analysis, LDA)
* LDA的思想：给定训练样例集，将样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。在对新样例进行分类时，先将其投影到该条直线上，再根据投影点的位置来确定新的类别
* 给定数据集\\(D=\lbrace(\vec{x} _i,y _i)\rbrace^m _{i=1}\\)，\\(y _i\in\lbrace 0, 1\rbrace\\)，令\\(\vec{X} _i、\vec{\mu} _i、\vec{\Sigma} _i\\)分别表示第\\(i\in\lbrace 0, 1\rbrace\\)类的实例集合、均值向量、协方差矩阵。若将数据投影到直线上，则两类样本的中心在直线上的投影分别为\\(\vec{\omega}^T\vec{\mu} _0和\vec{\omega}^T\vec{\mu} _1\\)，协方差分别为\\(\vec{\omega}^T\vec{\Sigma} _0\vec{\omega}和\vec{\omega}^T\vec{\Sigma} _1\vec{\omega}\\)，那么最大化目标为
$$
J=\frac{||\vec{\omega}^T\vec{\mu} _0-\vec{\omega}^T\vec{\mu} _1|| _2^2}{\vec{\omega}^T(\vec{\Sigma} _0+\vec{\Sigma} _1)\vec{\omega}}
$$
* 定义“类内散度矩阵”(witnin-class scatter matrix)：
$$
\begin{equation}
\begin{aligned}
\vec{S} _{\omega} &=\vec{\Sigma} _0+\vec{\Sigma} _1 \\\\
&=\sum _{\vec{x}\in X _0}(\vec{x}-\vec{\mu} _0)(\vec{x}-\vec{\mu} _0^T)+\sum _{\vec{x}\in X _1}(\vec{x}-\vec{\mu} _1)(\vec{x}-\vec{\mu} _1^T)
\end{aligned}
\end{equation}
$$
定义“类间散度矩阵”(between-class scatter matrix)：
$$
\vec{S} _b=(\vec{\mu} _0-\vec{\mu} _1)(\vec{\mu} _0-\vec{\mu} _1)^T
$$
最大化目标重写为：
$$
J=\frac{\vec{\omega}^T\vec{S} _b\vec{\omega}}{\vec{\omega}^T\vec{S} _{\omega}\vec{\omega}}
$$
* 注意到如果\\(\vec{\omega}\\)是一个解，那么\\(\alpha\vec{\omega}\\)也是一个解，故不失一般性，令\\(\vec{\omega}^T\vec{S} _{\omega}\vec{\omega}=1\\)，则由拉格朗日乘子法有$$$$
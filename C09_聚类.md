<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# C09-聚类
## 9.1 聚类任务
* 聚类将数据集中的样本划分为若干个通常是不想交的子集，每个子集称为一个簇(cluster)。假定样本集\\(D=\lbrace\vec{x} _1,\vec{x} _2,\cdots,\vec{x} _m\rbrace\\)包含*m*个无标记样本，每个样本\\(\vec{x} _i=(x _{i1};x _{i2};\cdots;x _{in})\\)是一个*n*维向量，则聚类算法将样本集*D*划分为*k*个不想交的簇\\(\lbrace C _l|l=1,2,\cdots,k\rbrace\\)，其中\\(C _{l'}\cap _{l'\ne l}C _l=\phi且D=\cup _{l=1}^kC _l\\)，相应的，用\\(\lambda _j\in\lbrace1,2,\cdots,k\rbrace\\)表示样本\\(\vec{x} _j\\)的簇标记(cluster label)，即\\(\vec{x} _j\in C _{\lambda _j}\\)，于是聚类的结果可以用*m*个元素的簇标记向量\\(\vec{\lambda}=(\lambda _1;\lambda _2;\cdots;\lambda _m)\\)
## 9.2 性能度量
* 聚类性能度量亦称为聚类有效性指标(validity index)。如果明确了性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果
* 好的聚类结果意味着“簇内相似度”(intra-cluster similarity)高且“簇间相似度”(inter-cluster similarity)低
* 聚类性能度量大致分为两类，一类是将聚类结果与某个“参考模型”(reference model)进行比较，称为外部指标(external index)，另一类是直接考察聚类结果而不利用任何参考模型，称为内部指标(internal index)
* 对于数据集\\(D=\lbrace\vec{x} _1,\vec{x} _2,\cdots,\vec{x} _m\rbrace\\)，假定通过聚类给出的簇划分为\\(C=\lbrace C _1,C _2,\cdots,C _k\rbrace\\)，参考模型给出的簇划分为\\(C^*=\lbrace C _1^*,C _2^*,\cdots,C _s^*\rbrace\\)，相应地，令\\(\vec{\lambda}和\vec{\lambda}^*\\)分别为对应的簇标记向量，将样本两两配对，定义
$$
\begin{aligned}
&a=|SS|,\quad SS=\lbrace(\vec{x} _i,\vec{x} _j)|\lambda _i=\lambda _j,\lambda _i^*=\lambda _j^*,i< j)\rbrace,\\\\
&b=|SD|,\quad SD=\lbrace(\vec{x} _i,\vec{x} _j)|\lambda _i=\lambda _j,\lambda _i^*\ne\lambda _j^*,i< j)\rbrace,\\\\
&c=|DS|,\quad DS=\lbrace(\vec{x} _i,\vec{x} _j)|\lambda _i\ne\lambda _j,\lambda _i^*=\lambda _j^*,i< j)\rbrace,\\\\
&d=|DD|,\quad DD=\lbrace(\vec{x} _i,\vec{x} _j)|\lambda _i\ne\lambda _j,\lambda _i^*\ne\lambda _j^*,i< j)\rbrace,\\\\
&a+b+c+d=\frac{m(m-1)}{2}
\end{aligned}
$$
根据上面的式子就可以导出以下常用的聚类性能度量外部指标。（1）Jaccard系数(Jaccard Coefficient,JC)：
$$
JC=\frac{a}{a+b+c}
$$
（2）FM指数(Fowlkes and Mallows Index，FMI)：
$$
FMI=\sqrt{\frac{a}{b+c}\cdot\frac{a}{a+c}}
$$
（3）Rand指数(Rand Index,RI)：
$$
RI=\frac{2(a+b)}{m(m-1)}
$$
上述性能度量在[0,1]之间，且值越大越好
* 考虑聚类结果簇划分\\(C=\lbrace C _1,C _2,\cdots,C _k\rbrace\\)，定义
$$
\begin{aligned}
avg(C)&=\frac{2}{|C|(|C|-1)}\sum _{1\le i< j\le|C|}dist(\vec{x} _i,\vec{x} _j),\\\\
diam(C)&=max _{1\le i< j\le|C|}dist(\vec{x} _i,\vec{x} _j),\\\\
d _{min}(C _i,C _j)&=min _{\vec{x} _i\in C _i,\vec{x} _j\in C _j}dist(\vec{x} _i,\vec{x} _j),\\\\
d _{cen}(C _i,C _j)&=dist(\vec{\mu} _i,\vec{\mu} _j),
\end{aligned}
$$
其中*dist*用于计算两个样本之间的距离，\\(\vec{\mu}\\)表示簇*C*的中心点\\(\vec{\mu}=\frac{1}{|C|}\sum _{1\le i\le|C|}\vec{x} _i\\)，有如下常用的聚类性能度量内部指标：（1）DB指数(Davies-Boildin Index,DBI)：
$$
DBI=\frac{1}{k}\sum _{i=1}^kmax _{j\ne i}\Big(\frac{avg(C _i)+avg(C _j)}{d _cen(\vec{\mu} _i,\vec{\mu} _j)}\Big)
$$
（2）Dunn指数(Dunn Index,DI)：
$$
DI=min _{1\le i\le k}\Big\lbrace min _{j\ne i}\Big(\frac{d _{min}(C _i,C _j)}{max _{1\le l\le k}diam(C _l)}\Big)\Big\rbrace
$$
DBI的值越小越好，DI的值越大越好

## 9.3 距离计算

* 对函数*dist*，若它是一个距离度量(distance measure)，则需要满足一些基本性质（1）非负性：\\(dist(\vec{x} _i,\vec{x} _j)\ge 0\\)（2）同一性：\\(dist(\vec{x} _i,\vec{x} _j)=0当且仅当\vec{x} _i=\vec{x} _j\\)（3）对称性：dist(\vec{x} _i,\vec{x} _j)=dist(\vec{x} _j,\vec{x} _i)（4）直抵性：\\(dist(\vec{x} _i,\vec{x} _j)\le dist(\vec{x} _i,\vec{x} _k)+dist(\vec{x} _k,\vec{x} _j)\\)

* 对于距离，最常用的是闵可夫斯基距离(Minkowski distance)：
$$
dist _{mk}(\vec{x} _i,\vec{x} _j)=\Big(\sum _{u=1}^n|x _{iu}-x _{ju}|^p\Big)^{\frac{1}{p}}
$$
* 属性可以分为连续属性(continuous attribute)和离散属性(categorical attribute)。如果属性定义了“序”的关系，那么就被称为有序属性，这种属性与连续属性更接近，可以直接使用闵可夫斯基距离计算。而无序属性则不可以直接使用闵可夫斯基距离计算
* 对于无序属性，可采用VDM(Value Difference Metric)，令\\(m _{u,a}\\)表示在属性*u*上取值为*a*的样本数，\\(m _{u,a,i}\\)表示在第*i*个样本簇中在属性*u*上取值为*a*的样本数，*k*为样本簇数，则 属性*u*上两个离散值*a*和*b*之间的VDM距离为
$$
VDM _p(a,b)=\sum _{i=1}{k}\Big|\frac{m _{u,a,i}}{m _{u,a}}-\frac{m _{u,b,i}}{m _{u,b}}\Big|^p
$$
* 通常，我们是基于某种形式的距离来定义相似度度量，距离越大，相似度越小。然而，用于相似度度量的距离未必一定要满足距离度量额所有基本性质，尤其是直递性。这样的距离称为“非度量距离”
* 在不少现实任务中，有必要基于数据样本来确定合适的距离计算式，这可通过“距离度量学习”来实现
## 9.4 原型聚类(prototype-based clustering)
* 原型聚类假设聚类结构能通过一组原型刻画，在现实聚类任务中极为常见。通常情况下，算法先对原型进行初始化，然后对原型进行迭代更新求解。
### 9.4.1 k均值算法
![](./picture/C09/kmeans.png)
### 9.4.2 学习向量化(Learning Vector Quantization,LVQ)
* 给定样本集\\(D=\lbrace(\vec{x} _1,y _1),(\vec{x} _2,y _2),\cdots,(\vec{x} _m,y _m)\rbrace\\)，每个样本\\(\vec{x} _j\\)是由*n*个属性描述的特征向量，\\(y _i\in\mathcal{Y}\\)是样本\\(\vec{x} _j\\)的类别标记。LVQ的目标是学得一组*n*维原型向量\\(\lbrace\vec{p} _1,\vec{p} _2,\cdots,\vec{p} _q\rbrace\\)，每个原型向量代表一个聚类簇，簇标记为\\(t _i\\)
![](./picture/C09/LVQ.png)
* 在学得一组原型向量\\(\lbrace\vec{p} _1,\vec{p} _2,\cdots,\vec{p} _q\rbrace\\)后，即可实现对样本空间的簇划分，对任意样本\\(\vec{x}\\)，它将被划入与其距离最近的原型向量所代表的簇中，换言之，每个原型向量\\(\vec{p} _i\\)定义了与之相关的一个区域\\(R _i\\)，该区域中每个样本与\\(\vec{p} _i\\)的距离不大于它与其他原型向量\\(\vec{p} _{i'}(i'\ne i)\\)的距离，即
$$
R _i=\big\lbrace \vec{x}\in\chi\big|||\vec{x}-\vec{p} _i|| _2\le ||\vec{x}-\vec{p} _{i'}|| _2, i'\ne i\big\rbrace
$$
### 9.4.3 高斯混合聚类
* 高斯混合聚类(Mixture-of-Gaussian)采用概率模型来表达聚类原型
* 多元高斯分布定义，对*n*维样本空间\\(\chi\\)中的随机向量\\(\vec{x}\\)，若\\(\vec{x}\\)服从高斯分布\\(\vec{x}\sim\mathcal{N}(\vec{\mu},\vec{\Sigma})\\)，其概率密度函数为
$$
p(\vec{x}|\vec{\mu},\vec{\Sigma})=\frac{1}{(2\pi)^{\frac{n}{2}}|\vec{\Sigma}|^\frac{1}{2}}e^{-\frac{1}{2}(\vec{x}-\vec{\mu})^T\vec{\Sigma}^{-1}(\vec{x}-\vec{\mu})}
$$
* 我们可以定义高斯混合分布
$$
p _{\mathcal{M}}(\vec{x})=\sum _{i=1}^k\alpha _i\dot p(\vec{x}|\vec{\mu} _i,\Sigma _i)
$$
该分布共由*k*个混合成分组成，每个混合成分对应一个高斯分布，每个高斯分布有其对应的混合系数(mixture coefficient),\\(\sum _{i=1}^k\alpha _i=1\\)
* 若训练集\\(D=\lbrace\vec{x} _1,\vec{x} _2,\cdots,\vec{x} _m\rbrace\\)由上述过程生成，令随机变量\\(z _j\in\lbrace1,2,\cdots,k\rbrace\\)表示生成样本\\(\vec{x} _j\\)的高斯混合成分，即\\(p(z _j=i)=\alpha _i(i=1,2,\cdots,k)\\)，根据贝叶斯定理，\\(z _j\\)的后验概率对应于
$$
\begin{aligned}
p _{\mathcal{M}}(z _j=i|\vec{x} _j)&=\frac{p(z _j=i)\dot p _(\vec{x} _j|z _j=i)}{p _\mathcal{M}(\vec{x} _j)}\\\\
&=\frac{\alpha _i\cdot p(\vec{x} _j|\vec{\mu} _i,\vec{\Sigma} _i)}{\sum _{l=1}^k\alpha _l\cdot p(\vec{x} _j|\vec{\mu} _l,\vec{\Sigma} _l)}\\\\
&\coloneqq\mathcal{\gamma} _{ji}
\end{aligned}
$$
* 高斯混合聚类将样本集*D*划分为*k*个簇\\(\mathcal{C}=\lbrace C _1,C _2,\cdots,C _k\rbrace\\)，每个样本\\(\vec{x} _j\\)的簇标记\\(\lambda _j\\)由\\(\lambda _j=argmax _{i\in\lbrace1,2,\cdots,k\rbrace}\mathcal{\gamma} _{ji}\\)确定
* 给定样本集*D*，可采用极大似然估计，即最大化对数似然
$$
\begin{aligned}
LL(D)&=ln\Big(\prod _{j=1}^mp _{\mathcal{M}}(\vec{x} _j)\Big)\\\\
&=\sum _{j=1}^mln\Big(\sum _{i=1}^k\alpha _i\cdotp(\vec{x} _j|\vec{\mu} _i,\vec{\Sigma} _i)\Big)
\end{aligned}
$$
有
$$
\begin{aligned}
\vec{\mu} _i&=\frac{\sum _{j=1}^m\mathcal{\gamma} _{ji}\vec{x} _j}{\sum _{j=1}^m\mathcal{\gamma} _{ji}}\\\\
\vec{\Sigma} _i&=\frac{\sum _{j=1}^m\mathcal{\gamma} _{ji}(\vec{x} _j-\vec{\mu} _i)(\vec{x} _j-\vec{\mu} _i)^T}{\sum _{j=1}^m\mathcal{\gamma} _{ji}}\\\\
\alpha _i&=\frac{1}{m}\sum _{j=1}^m\mathcal{\gamma} _{ji}
\end{aligned}
$$
![](./picture/C09/MOG.png)
## 9.5 密度聚类(density-based clustering)
* 此类算法假定聚类结构能通过样本分布的紧密程度确定。通常情况下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇来获得最终聚类结果
* DBSCAN(Density-Based Spatial Clustering of Applications with Noise)是一种著名的密度聚类算法，它基于一组邻域(neighborhood)参数\\((\epsilon,MinPts)\\)来刻画样本分布的紧密程度。
* 给定数据集\\(D=\lbrace\vec{x} _1,\vec{x} _2,\cdots,\vec{x} _m\rbrace\\)定义概念
(1)\\(\epsilon-\\)邻域：对\\(\vec{x} _j\in D\\)，其\\(\epsilon-\\)邻域包含样本集*D*中与\\(\vec{x} _j\\)的距离不大于\\(\epsilon\\)的样本，即\\(N _{\epsilon}(\vec{x} _j)=\lbrace\vec{x} _j\in D|dist(\vec{x} _i,\vec{x} _j)\le\epsilon\rbrace\\)
(2)核心对象(core object)：若\\(\vec{x} _j\\)的\\(\epsilon-\\)邻域至少包含*MinPts*个样本，即\\(|N _{\epsilon}(\vec{x} _j)|\ge MinPts\\)，则\\(\vec{x} _j\\)是一个核心对象
(3)密度直达(directly density-reachable)：若\\(\vec{x} _j\\)位于\\(\vec{x} _i\\)的\\(\epsilon-\\)邻域中，且\\(\vec{x} _i\\)是核心对象，则称\\(\vec{x} _j\\)由\\(\vec{x} _i\\)密度直达，密度直达通常不满足对称性
(4)密度可达(density-reachable)：对\\(\vec{x} _i和\vec{x} _j\\)，若存在样本序列\\(\vec{p} _1,\vec{p} _2,\cdots,\vec{p} _n\\)，其中\\(\vec{p} _1=\vec{x} _i,\vec{p} _n=\vec{x} _j且\vec{p} _{i+1}由\vec{p} _{i}密度直达\\)，则称\\(\vec{x} _j\\)由\\(\vec{x} _i\\)密度可达
(5)密度相连(density-connected)：对\\(\vec{x} _i与\vec{x} _j\\)，若存在\\(\vec{x} _k\\)使得\\(\vec{x} _i与\vec{x} _j\\)均由\\(\vec{x} _k\\)密度可达，则称\\(\vec{x} _i和\vec{x} _j\\)密度相连
* DBSCAN将簇的概念定义为：由密度可达关系导出的最大密度相连样本的集合,*D*中不属于任何簇的样本会被认为是噪声
## 9.6 层次聚类(hierarchical clustering)
* 层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可以采用“自顶向下”的分拆策略
* AGNES(AGglomerative NESting)是一种采用自底向上的聚合策略的层次聚类算法。它先将每个样本看做一个初始的聚类簇，然后在算法运行每一步找出距离最近的两个聚类簇进行合并，该过程不断重复，直到达到预设的聚类簇个数
* 当聚类簇距离由\\(d _{min},d _{max},d _{avg}\\)计算时，AGNES算法被相应的称为单链接(single-linkage)、全连接(complete-linkage)或均连接(average-linkage)
![](./picture/C09/AGNES.png)
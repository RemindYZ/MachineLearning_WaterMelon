<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# C06-支持向量机
## 6.1 间隔和支持向量
* 给定训练样本集\\(D=\lbrace(\vec{x} _1,y _1),(\vec{x} _2,y _2),\cdots,(\vec{x} _m,y _m)\rbrace, y _i\in\lbrace -1, +1\rbrace\\)，分类学习最基本的想法就是基于训练集*D*在样本空间找到一个划分超平面，将不同类别的样本分开
* 在样本空间中，划分超平面可以通过如下方程式来描述
\begin{aligned}
\vec{\omega}^T\vec{x}+b=0
\end{aligned}
其中，\\(\vec{\omega}=(\omega _1,\omega _2,\cdots,\omega _d)\\)为法向量，决定了超平面的方向，*b*为位移项，决定了超平面与原点之间的距离。样本空间的任意点\\(\vec{x}\\)到超平面\\((\vec{\omega},b)\\)的距离可以写为
\begin{aligned}
r=\frac{|\vec{\omega}^T\vec{x}+b|}{||\vec{\omega}||}
\end{aligned}
![](.\picture\C06\supportvectorandmargin.png)
* 实际上，需要：
\begin{equation}
\begin{aligned}
& max _{\vec{\omega},b}\quad \frac{2}{||\vec{\omega}||} \\\\
& s.t.\quad y _i(\vec{\omega}^T\vec{x} _i+b)\ge 1, \quad i=1,2,\cdots,m
\end{aligned}
\end{equation}
可以重写为
\begin{equation}
\begin{aligned}
& min _{\vec{\omega},b}\quad \frac{1}{2}||\vec{\omega}||^2 \\\\
& s.t.\quad y _i(\vec{\omega}^T\vec{x} _i+b)\ge 1, \quad i=1,2,\cdots,m
\end{aligned}
\end{equation}
这就是SVM的基本型
## 6.2 对偶问题
* 使用拉格朗日乘子法可得到对偶问题(dual problem)
\begin{aligned}
L(\vec{\omega},b,\vec{\alpha})=\frac{1}{2}||\vec{\omega}||^2+\sum _{i=1}^m\alpha _i(1-y _i(\vec{\omega}^T\vec{x} _i+b))
\end{aligned}
其中拉格朗日乘子\\(\alpha _i\ge 0,\vec{\alpha}=(\alpha _1,\alpha _2,\cdots,\alpha _m)\\)，对\\(\vec{\omega}和b\\)求偏导，得到对偶问题：
\begin{equation}
\begin{aligned}
& max _\vec{\alpha}\quad\sum _{i=1}^m\alpha _i-\frac{1}{2}\sum _{i=1}^m\sum _{j=1}^m\alpha _i\alpha _jy _iy _i\vec{x} _i^T\vec{x} _j \\\\
& s.t \quad \sum _{i=1}^m\alpha _iy _i=0 \\\\
& \qquad\alpha _i\ge 0, \quad i=1,2,\cdots,m
\end{aligned}
\end{equation}
* SMO(Sequential Minimal Optimization)算法：不断执行如下的两个步骤直到收敛：（1）选取一对需要更新的变量\\(\alpha _i和\alpha _j\\)；（2）固定\\(\alpha _i和\alpha _j\\)以外的参数，求解对偶问题获得更新后的\\(\alpha _i和\alpha _j\\)
* 对于偏移量*b*通常采用所有的支持向量求解平均值\\(b=\frac{1}{|S|}\sum _{s\in S}(\frac{1}{y _s}-\sum _{i\in S}\alpha _iy _i\vec{x} _i^T\vec{x} _s)\\)
## 6.3 核函数
![](.\picture\C06\kernelfunction.png)
* 线性不可分的问题可以通过将样本空间映射到一个更高维度的特征空间，使得样本在这个特征空间内线性可分。如果原始空间是有限维的，那么一定存在一个高纬度空间使样本可分
* 令\\(\phi(\vec{x})\\)表示将\\(\vec{x}\\)映射后的特征向量，设想一个函数\\(\kappa(\vec{x} _i,\vec{x} _j)=\langle\phi(\vec{x} _i),\phi(\vec{x} _j)\rangle=\phi(\vec{x} _i)^T\phi(\vec{x} _j)\\)，称之为核函数，通过原始样本空间的向量来求解特征空间的内积
* 令\\(\chi\\)为输入空间，\\(\kappa(·,·)\\)是定义在\\(\chi\times\chi\\)上的对称函数，则\\(\kappa\\)是核函数当且仅当对于任意数据\\(D=\lbrace\vec{x} _1,\vec{x} _2,\cdots,\vec{x} _m\rbrace\\)，核矩阵(kernel matrix)\\(\vec{K}\\)总是半正定的：
$$
\vec{K}=\begin{pmatrix}
        \kappa(\vec{x} _1,\vec{x} _1) & \cdots & \kappa(\vec{x} _1,\vec{x} _j) & \cdots & \kappa(\vec{x} _1,\vec{x} _m) \\\\
        \vdots & \ddots & \vdots & \ddots & \vdots\\\\
        \kappa(\vec{x} _i,\vec{x} _1) & \cdots & \kappa(\vec{x} _i,\vec{x} _j) & \cdots & \kappa(\vec{x} _i,\vec{x} _m) \\\\
        \vdots & \ddots & \vdots & \ddots & \vdots\\\\
        \kappa(\vec{x} _m,\vec{x} _1) & \cdots & \kappa(\vec{x} _m,\vec{x} _j) & \cdots & \kappa(\vec{x} _m,\vec{x} _m) \\\\
        \end{pmatrix}
$$
* 常用核函数
![](.\picture\C06\tableofkernelsfunction.png)
此外还可以通过函数组合得到，例如
（1）若\\(\kappa _1\\)和\\(\kappa _2\\)均为核函数，则对于任意正数\\(\gamma _1\\)和\\(\gamma _2\\)，其线性组合\\(\gamma _1\kappa _1+\gamma _2\kappa _2\\)也是核函数（2）若\\(\kappa _1\\)和\\(\kappa _2\\)均为核函数，则核函数的直积\\(\kappa _1\bigotimes\kappa _2(\vec{x},\vec{v})=\kappa _1(\vec{x}, \vec{z})\kappa _2(\vec{x},\vec{z})\\)也是核函数（3）若\\(\kappa _1\\)为核函数，则对于任意函数\\(g(\vec{x})\\)，\\(\kappa(\vec{x},\vec{z})=g(\vec{x})\kappa _1(\vec{x},\vec{z})g(\vec{z})\\)也是核函数
## 6.4 软间隔与正则化
* 为了提高泛化能力，我们允许支持向量机在一些样本上出错，即允许某些样本不满足约束\\(y _i(\vec{\omega}^T\vec{x} _i+b)\ge 1\\)，在最大化间隔的同时，不满足约束的样本尽可能少，于是，优化目标可以写为
\begin{aligned}
min _{\vec{\omega},b} \quad\frac{1}{2}||\vec{\omega}||^2+C\sum _{i=1}^ml _{0/1}(y _i(\vec{\omega}^T\vec{x} _i+b)-1)
\end{aligned}
其中，\\(C>0\\)是一个常数，\\(l _{0/1}\\)是“0/1损失函数”
\begin{aligned}
l _{0/1}(z)=
\begin{cases}
&1,\quad &if\quad z<0\\\\
&0,\quad &otherwise
\end{cases}
\end{aligned}
* “0/1损失函数”非凸、非连续、数学性质不太好，常用一些函数来代替
![](.\picture\C06\lossfunction.png)
* 引入松弛变量(slack variables)\\(\xi _i\ge0\\)，可以得到常用的软间隔支持向量机
\begin{aligned}
min _{\vec{\omega},b,\xi _i}\quad&\frac{1}{2}||\vec{\omega}||^2+C\sum _{i=1}^m\xi _i \\\\
s.t. \quad &y _i(\vec{\omega}^T\vec{x} _i+b)\ge1-\xi _i \\\\
&\xi _i\ge0,\quad i=1,2,\cdots,m
\end{aligned}
其对偶问题为
\begin{aligned}
max _{\vec{\alpha}}\quad&\sum _{i=1}^m\alpha _i-\frac{1}{2}\sum _{i=1}^m\sum _{j=1}^m\alpha _i\alpha _jy _iy _j\vec{x} _i^T\vec{x} _j \\\\
s.t.\quad &\sum _{i=1}^m\alpha _iy _i=0 \\\\
&0\le\alpha _i\le C,\quad i=1,2,\cdots,m
\end{aligned}
* 优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项用来表述训练集上的误差。前者称为结构风险(structural risk)，用于描述模型的性质，后者称为经验风险，用于描述模型与训练数据的契合程度，使用系数进行折中。前者称为正则化项，一般用范数来表达，其中二范数倾向于分量取值均衡，而无穷范数与一范数则倾向于分量尽量稀疏
## 6.5 支持向量回归(SVR)
* 假设我们容忍\\(f(\vec{x})\\)与\\(y\\)之间最多有\\(\epsilon\\)
\begin{aligned}
min _{\vec{\omega},b}\quad\frac{1}{2}||\vec{\omega}||^2+C\sum _{i=1}^{m}l _\epsilon(f(\vec{x} _i)-y _i)
\end{aligned}
其中C为正则化常数，\\(l _\epsilon\\)是\\(\epsilon\\)-insensitive loss函数：
\begin{aligned}
l _\epsilon(z)=
\begin{cases}
&0, \quad &if\quad |z|\le\epsilon \\\\
&|z|-\epsilon, &otherwise
\end{cases} 
\end{aligned}
由于间隔带两侧的松弛程度可以不一样，引入松弛变量\\(\xi _i和\hat{\xi} _i\\)，有：
\begin{aligned}
min _{\vec{\omega},b,\xi _i,\hat{\xi} _i}\quad&\frac{1}{2}||\vec{\omega}||^2+C\sum _{i=1}^m(\xi _i+\hat{\xi} _i) \\\\
s.t. \quad & f(\vec{x} _i)-y _i\le\epsilon+\xi _i,\\\\
& y _i-f(\vec{x} _i)\le\epsilon+\hat{\xi} _i,\\\\
& \xi _i\ge0,\hat{\xi} _i\ge0,\quad i=1,2,\cdots,m.
\end{aligned}
其对偶问题为：
\begin{aligned}
max _{\vec{\alpha},\hat{\vec{\alpha}}}\quad&\sum _{i=1}^my _i(\hat{\alpha} _i-\alpha _i)-\epsilon(\hat{\alpha} _i+\alpha _i)-\frac{1}{2}\sum _{i=1}^m\sum _{j=1}^m(\hat{\alpha} _i-\alpha _i)(\hat{\alpha} _j-\alpha _j)\vec{x} _i^T\vec{x} _j \\\\
s.t.\quad&\sum _{i=1}^m(\hat{\alpha} _i-\alpha _i)=0, \\\\
&0\le\alpha _i,\hat{\alpha} _i\le C
\end{aligned}
满足KKT条件
\begin{aligned}
\begin{cases}
\alpha _i(f(\vec{x} _i)-y _i-\epsilon-\xi _i)=0,\\\\
\hat{\alpha} _i(y _i-f(vec{x} _i)-\epsilon-\hat{\xi} _i)=0,\\\\
\alpha _i\hat{\alpha} _i=0,\xi _i\hat{\xi} _i=0,\\\\
(C-\alpha _i)\xi _i=0,(C-\hat{\alpha} _i)\hat{\xi} _i=0
\end{cases}
\end{aligned}
## 6.6 核方法

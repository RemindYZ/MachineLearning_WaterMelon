<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# C05-神经网络
## 5.1 神经元模型
* 神经网络中最基本的成分是神经元(neuron)模型

![](.\picture\C05\neuron.PNG)
* 在这个模型中，神经元接收到来自n个其它神经元传递过来的输入信号，这些输入通过带权值的连接进行传递，神经元接收到的总输入将值将与神经元的阈值进行比较，然后通过激活函数(activation function)处理以产生神经元的输出
## 5.2 感知机和多层网络
* 感知机(perceptron)由两层神经元组成，输入层接受外界输入信号后传递给输出层，输出层是M-P神经元，也叫阈值逻辑单元(threshold logic unit)
* 感知机能容易地实现逻辑与、或、非运算
* 更一般的，给定训练数据集，权重\\(\omega _i(i=1,2,\cdots ,n)\\)，以及阈值\\(\theta\\)可通过学习得到，阈值可以看作一个固定输入为-1的哑结点(dummy node)所对应的连接权重\\(\omega _{n+1}\\)
* 对训练样例\\((\vec{x},y)\\)，若当前感知机的输出为\\(\hat{y}\\)，则感知机权重将这样调整：
$$
\omega _i\gets \omega _i+\Delta\omega _i \\\\
\Delta\omega _i=\eta (y-\hat{y})x _i
$$
其中，\\(\eta\in (0,1)\\)称为学习率(learning rate)。
* 感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元(functional neuron)，学习能力非常有限。
* 要解决非线性可分问题，需考虑使用多层功能神经元。在输入层与输出层之间加入隐层(hidden layer)，隐层和输出层神经元都是拥有激活函数的功能神经元
* 常见的神经网络是层级结构，每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经元结构通常称为多层前馈神经网络(multi-layer feedforward neural networks)，前馈并不意味着网络信号不能向后传，而是指网络拓扑结构不存在环或回路。
* 在神经网络的学习过程中，就是根据训练数据来调整神经元之间的连接权以及每个功能神经元的阈值。
## 5.3 误差逆传播算法
* BP(back propagation)算法，不仅可用于多层前馈神经网络，还可以用于其他类型的神经网络。
* 给定训练集\\(D=\lbrace (\vec{x} _1,\vec{y} _1),(\vec{x} _2,\vec{y} _2),\cdots ,(\vec{x} _m,\vec{y} _m)\rbrace ,\vec{x} _i\in R^d,\vec{y} _i\in R^l\\)，给出一个拥有*d*个输入神经元、*l*个输出神经元、*q*个隐层神经元的多层前馈网络结构，其中输入层第*j*个神经元的阈值用\\(\theta _j\\)表示，隐层第*h*个神经元的阈值用\\(\gamma _h\\)表示，输入层第*i*个神经元与隐层第*h*个神经元之间的连接权为\\(v _{ih}\\)，隐层第*h*个神经元与输出层第*j*个神经元之间的连接权威\\(\omega _{hj}\\)。记隐层第*h*个神经元接收到的输入为\\(\alpha =\sum _{i=1}^dv _{ih}x _i\\)，输出层第*j*个神经元接收到的输入为\\(\beta _j=\sum _{h=1}^q\omega _{hj}b _h\\)，假设隐层和输出层的神经元都使用Sigmoid函数\\(y=\frac{1}{1+e^{-z}}\\)。

![](.\picture\C05\BP.png)
* 对训练样例\\(\vec{x} _k,\vec{y} _k\\)，假定神经网络的输出为\\(\hat{\vec{y}} _k=(\hat{y} _1^k,\hat{y} _2^k,\cdots ,\hat{y} _l^k)\\)，即
$$
\hat{y} _j^k=f(\beta _j-\theta _j )
$$
则网络在\\((\vec{x} _k,\vec{y} _k)\\)上的均方误差为
$$
E _k=\frac{1}{2}\sum _{j=1}^l(\hat{y}^k _j-y^k _j)^2
$$
这里的1/2是为了后面求导方便
* BP算法基于梯度下降策略(gradient descent)，以目标的负梯度方向对参数进行调整，对误差\\(E _k\\)，给定学习率\\(\eta\\)，有
$$
\Delta \omega _{hj}=-\eta\frac{\partial E _k}{\partial\omega _{hj}}
$$
根据链式法则
$$
\frac{\partial E _k}{\partial\omega _{hj}}=\frac{\partial E _k}{\partial\hat{y}^k _j}\frac{\partial\hat{y}^k _j}{\partial\beta _j}\frac{\partial\beta _j}{\partial\omega _{hj}}
$$
根据\\(\beta _j\\)的定义，显然有
$$
\frac{\partial\beta _j}{\partial\omega _{hj}}=b _h
$$
而Sigmoid函数有一个很好的性质\\(f'(x)=f(x)(1-f(x))\\)，就有：
$$
\begin{equation}
\begin{aligned}
g _j &= -\frac{\partial E _k}{\partial\hat{y}^k _j}\frac{\partial\hat{y}^k _j}{\partial\beta _j} \\\\
&= -(\hat{y}^k _j-y^k _j)f'(\beta _j-\theta _j) \\\\
&= \hat{y}^k _j(1-\hat{y}^k _j)(y^k _j-\hat{y}^k _j)
\end{aligned}
\end{equation}
$$
就能得到BP算法中的更新公式
$$
\begin{equation}
\begin{aligned}
\Delta\omega _{hj}&=\eta g _jb _h \\\\
\Delta\theta _j &=-\eta g _j \\\\
\Delta v _{ih} &=\eta e _hx _i \\\\
\Delta\gamma _h &=-\eta e _h \\\\ 
\end{aligned}
\end{equation}
$$
其中
$$
\begin{equation}
\begin{aligned}
e _h &=-\frac{\partial E _k}{\partial b _h}\frac{\partial b _h}{\partial\alpha _h} \\\\
&= -\sum _{j=1}^l\frac{\partial E _k}{\partial\beta _j}\frac{\partial\beta _j}{\partial b _h}f'(\alpha _h-\gamma _h) \\\\
&=\sum _{j=1}^l\omega _{hj}g _jf'(\alpha _h-\gamma _h) \\\\
&=b _h(1-b _h)\sum _{j=1}^l\omega _{hj}g _j 
\end{aligned}
\end{equation}
$$
* 学习率\\(\eta\in (0,1)\\)控制着每一轮迭代的更新步长，有时每一层的学习率也会设置成不一样

![](.\picture\C05\BP_algorithm.png)
* 需要注意的是，BP算法的目标是要最小化训练集*D*上的累积误差
$$
E=\frac{1}{m}\sum _{k=1}^mE _k
$$
标准BP算法每次针对一个训练样例更新连接权值和阈值，如果根据累计误差来推导，可以得到累积误差逆传播(accumulated error backpropagation)算法。累计BP算法与标准BP算法都很常用。标准BP算法往往需要进行更多次数的迭代，累计BP算法更新的频率低得多。但在很多任务中，累计误差下降到一定程度后，进一步下降会非常缓慢，这时标准BP算法往往会更快获得较好的解，尤其是训练集*D*非常大时更明显
* BP网络常常遇到过拟合，一种策略是“早停”(early stopping)，另一种策略是“正则化”(regularization)，\\(E=\lambda\frac{1}{m}\sum _{k=1}^mE _k+(1-\lambda)\sum _i\omega^2 _i\\)。
## 5.4 全局最小和局部最小
* 以多组不同的参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数
* 使用模拟退火(simulated annealing)技术，模拟退火在每一步都有一定的概率接受比当前解更差的结果，从而有助于跳出局部最小，在每步迭代中接受次优解的概率要随着时间的推移而逐渐降低，从而保证算法的稳定
* 使用随机梯度下降
## 5.5 其他常见神经网络
### 5.5.1 RBF网络
* RBF(Radial Basis Function)网络是一种单隐层前馈神经网络，使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。假定输入为*d*维向量\\(\vec{x}\\)，输出为实值，则RBF网络可表示为
$$
\phi(\vec{x})=\sum _{i=1}^q\omega _i\rho(\vec{x},\vec{c} _i)
$$
其中*q*为隐层神经元个数，\\(\vec{c} _i和\omega _i\\)分别是第*i*个隐层神经元所对应的中心和权重，\\(\rho(\vec{x},\vec{c} _i)\\)是径向基函数，这是某种沿径向对称的标量函数，通常定义为样本\\(\vec{x}\\)到数据中心\\(\vec{c} _i\\)之间欧氏距离的单调函数。常用的高斯径向基函数形如：
$$
\rho(\vec{x},\vec{c} _i)=e^{-\beta _i||\vec{x}-\vec{c} _i||^2}
$$
足够多隐层神经元的RBF网络能以任意精度逼近任意连续函数。通常采用两步来训练RBF网络：第一步，确定神经元中心\\(\vec{c} _i\\)，常用的方式包括随机采样、聚类等；第二步，利用BP算法等来确定参数\\(\omega _i和\beta _i\\)
### 5.5.2 ART网络
* 竞争型学习是神经网络中一种常用的无监督学习策略，在使用该策略中时，网络的输出神经元互相竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制。这种机制亦称为“胜者通吃”(winner-take-all)原则
* ART(Adaptive Resonance Theory，自适应谐振理论)网络是竞争学习的重要代表，该网络由比较层、识别层、识别阈值和重置模块构成，其中比较层负责接收输入样本，并将其传递给识别层神经元，识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增加以增长新的模式类。
* ART网络具有可进行增量学习(incremental learning)或在线学习(online learning)这样一个优点
### 5.5.3 SOM网络
* SOM(Self-Organizing Map，自组织映射)网络是一种竞争型学习的无监督神经网络，它能将高维输入数据映射到低维空间，同时保持输入数据的在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元
* SOM训练过程很简单，在接受到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元(best matching unit)。然后最佳匹配单元及其邻近神经元的权向量将调整，以使得这些权向量与当前输入样本的距离缩小。这个过程不断迭代、直至收敛
### 5.5.4 级联相关网络
* 一般的神经网络模型通常假定网络结构是事先固定的，训练的目的是利用训练样本来确定合适的连接权、阈值等参数。与此不同，结构自适应网络则将网络结构也当做学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构。级联相关(Cascade-Correlation)网络是结构自适应网络的重要代表。
### 5.5.5 Elman网络
* 递归神经网络(recurrent neural networks)允许网络中出现环状结构，从而可让一些神经元的输出反馈回来作为输入信号，这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化
* Elman网络的隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元的下一时刻的输入，隐层神经元通常采用Sigmoid函数，而网络训练则常通过推广的BP算法进行
### 5.5.6 Boltzmann机
* 神经网络中有一类模型是为网络状态定义一个能量(energy)，能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数
* Boltzmann机就是一种基于能量的模型(energy-based model，其神经元分为两层：显层与隐层。显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达，Boltzmann机中的神经元都是布尔型的，即只取0，1两种状态，分别表示抑制和激活，
* 标准的Boltzmann机是一个全连接图，训练复杂度很高，现实中常采用受限Boltzmann机。仅保留显层与隐层的连接。受限Boltzmann机常用对比散度(Contrastive Divergence)算法进行训练。
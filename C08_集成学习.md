<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# C08-集成学习
## 8.1 个体与集成
* 集成学习通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统(multi-classifier system)、基于委员会的学习(committee-based learning)
* 同质(homogeneous)集成里个体学习器类型一样，亦称为基学习器(base learner)，相应的学习算法称为基学习算法(base learning algorithm)。异质(heterogenous)集成中的个体学习器由不同的学习算法生成，常称为组件学习器(component learner)或直接称为个体学习器
![](./picture/C08/ensemble.png)
* 要获得好的集成，个体学习器应该好而不同，即个体学习器要有一定的准确性和多样性(通常是矛盾的)。集成学习研究的核心就是如何产生并结合好而不同的个体学习器
* 根据个体学习器的生成方式，集成学习的方法大致可以分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法。前者的代表是Boosting，后者的代表是Bagging和随机森林Random Forest
## 8.2 Boosting
* 工作机制：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器，直到基学习器数目达到事先指定的值*T*，最终将这*T*个基学习器进行加权结合
* AdaBoost，其中\\(y _i\in\lbrace-1,+1\rbrace,f\\)是真实函数
![](./picture/C08/AdaBoost.png)
* Boosting算法要求基学习器能对特定的数据分布进行学习，这可通过赋权法进行，对于无法接受带权样本的基学习算法，则可通过重采样法(re-sampling)来处理。
* 每次生成的的基学习器需要验证当前生成的基学习器是否满足基本条件。
## 8.3 Bagging 和随机森林
### 8.3.1 Bagging
* 集成中的个体学习器应该尽可能相互独立。虽然独立在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生不同的子集，再使用不同的子集训练不同的基学习器。划分子集的办法可以使用自主采样法(bootstrap sampling)。给定m个样本的数据集，可以采集到*T*个含有*m*个样本的采样集，然后基于每个采样集训练出一个基学习器，再 将这些基学习器进行结合，就是Bagging的基本流程，对分类任务使用简单投票法，对回归任务使用简单平均法
![](./picture/C08/Bagging.png)
* Bagging方法主要关注降低方差，因此它在不剪枝决策树、神经网络等易受到样本扰动的学习器上效用更为明显
### 8.3.2 随机森林
* RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性选择。即在决策树算法决定节点属性时并不是采用全部属性，而是采用全部属性的一个子集来进行选择(引入随机性)，假定全部属性的数目为*d*，子集中属性的个数为*k*个，推荐\\(k=log _2(d)\\)
* 与Bagging不同的是，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升
## 8.4 结合策略
* 学习器结合可能会从三个方面带来好处（1）从统计方面来看，由于学习任务的假设空间很大，可能有多个假设咋训练集上达到同等性能，此时若使用单学习器可能因为误选而导致泛化性能不佳，结合多个学习器会减少这一风险（2）学习算法往往会陷入局部最小值，有的局部最小值对应的泛化性能非常糟糕（3）结合多个学习器，将对应的假设空间扩大，有可能学得更好的近似
* 假定集成学习包含*T*个基学习器\\(\lbrace h _1,h _2,h _3,\cdots,h _T\rbrace\\)
### 8.4.1 平均法
* 对数值型输出\\(h _i(\vec{x})\in R\\)，最常见的结合策略是使用平均法(averaging)
* 简单平均法(simple averaging)
$$
H(\vec{x})=\frac{1}{T}\sum _{i=1}^Th _i(\vec{x})
$$
* 加权平均法(weighted averaging)
$$
H(\vec{x})=\sum _{i=1}^T\omega _ih _i(\vec{x})
$$
其中\\(\omega _i\\)是个体学习器\\(h _i\\)的权重，通常要求\\(\omega _i\ge 0,\sum _{i=1}^T\omega _i=1\\)
加权平均法可以认为是集成学习研究的基本出发点，对给定的基学习器，不同的集成学习方法可视为通过不同的方式来确定加权平均法中的基学习器权重
* 由于训练样本可能出现的噪声或者不充分，使得学习出来的权重不一定可靠，因此加权平均法不一定优于简单平均法。一般而言，个体学习器性能相差较大时宜采用加权平均法，而在个体学习器性能相近时宜采用简单平均法
### 8.4.2 投票法
* 对于分类问题，一般采用的方法是投票法
* 绝对多数投票法(majority voting)，若某标记得票数过半，则预测为该标记，否则拒绝预测
* 相对多数投票法(plurality voting)，预测结果为得票最多的标记，若同时有多个标记获得最高票，则从中随机选取一个
* 加权投票法(weighted voting)
* 这些投票方法中并没有限制个体学习器的输出类型，一般而言有两种，一种是类标记，称为硬投票，一种是累概率，称为软投票。不同类型的输出不能混用。
### 8.4.3 学习法
* 在训练数据很多时，学习法是一种更为强大的结合策略，即通过另一个学习器来进行结合。Stacking是学习法的典型代表，这里我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器(meta-learner)
![](./picture/C08/Stacking.png)
* 将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归(Multi-response Linear Regression,MLR)作为次级学习算法效果较好。MLR是基于线性回归的的分类器，它对每个类分别进行线性回归，属于该类的训练样例所对应的输出被置为1，其他类置为0；测试示例将被分给输出值的最大类
## 8.5 多样性
### 8.5.1 误差-分歧分解
* 误差-分歧分解(error-ambiguity decomposition)个体学习器准确性越高，多样性越大，则集成越好
### 8.5.2 多样性度量
* 多样性度量(diversity measure)是用于度量集成个体分类器的多样性，即估算个体学习器的多样化程度
* 典型的做法是考虑个体分类器的两两相似和不相似性
![](./picture/C08/continegency_table.png)
* 常见的多样性度量：（1）不合度量(disagreement measure)：
$$
dis _{ij}=\frac{b+c}{m}
$$
（2）相关系数(correlation coefficient)：
$$
\rho _{ij}=\frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}
$$
（3）Q-统计量(Q-statistic)：
$$
\begin{aligned}
&Q _{ij}=\frac{ad-bc}{ad+bc}\\\\
&|Q _{ij}|\le|\rho _{ij}|
\end{aligned}
$$
（4）\\(\kappa _{ij}-统计量(\kappa -statistic)\\)：越大说明分类器一致性越高
$$
\begin{aligned}
&\kappa =\frac{p _1-p _2}{1-p _2} \\\\
&p _1=\frac{a+d}{m} \\\\
&p _2=\frac{(a+b)(a+c)+(c+d)(b+d)}{m^2}
\end{aligned}
$$
### 8.5.3 多样性增强
* 在集成学习中需要有效地生成多样性大的个体学习器，一般思路是在学习过程中引入随机性，常见的做法是对数据样本、输入属性、输出表示、算法参数进行扰动
* 数据样本扰动：给定初始数据集，从中产生出不同的数据子集，再利用不同的数据子集训练出不同的个体学习器，数据扰动通常是基于采样法。此类方法简单高效，使用很广，对很多常见的基学习器，例如决策树、神经网络等，训练样本稍加变化就会导致学习器有明显变动，数据采样扰动对这样的不稳定基学习器很有效。但是对于线性学习器、支持向量机、朴素贝叶斯、k近邻学习器等，这些学习器称为稳定学习器，效果不好，需要其他的扰动机制。
* 输入属性扰动：随机子空间(random subspace)算法。对包含大量冗余属性的数据，在子空间中训练个体学习器不仅能产生多样性大的个体，还会因为属性减少而大幅节省时间开销。若数据只包含少量属性或者冗余属性很少，则不宜使用输入属性扰动法
![](./picture/C08/RandomSubspace.png)
* 输出表示扰动：基本思路是对输出表示进行操纵以增强多样性，可对训练样本的类标记稍作变动，如翻转法(Flipping Output)随机改变一些训练样本的标记，也可以对输出表示进行转化，如输出调制法(Output Smearing)将分类输出转化为回归输出后构建个体学习器；还可以将原任务拆解为多个可同时求解的子任务，如ECOC法，利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器
* 算法参数扰动：通过随机设置不同的参数，往往可以产生差别较大的个体学习器，例如负相关法(Negative Correlation)显式地通过正则化项来强制个体神经网络使用不同的参数。对参数较少的算法，可通过将其学习过程中某些环节用其他方式代替，从而达到扰动的目的
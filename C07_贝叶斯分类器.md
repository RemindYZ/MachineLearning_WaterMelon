<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# C07-贝叶斯分类器
## 7.1 贝叶斯决策论(Bayesian decision theory)
* 假设有*N*种可能的类别标记，即\\(y=\lbrace c _1,c _2,\cdots,c _N\rbrace\\)，\\(\lambda _{ij}\\)是将一个真实标记为\\(c _j\\)的样本误分类为\\(c _i\\)所产生的损失。基于后验概率\\(P(c _i|\vec{x})\\)可获得将样本\\(\vec{x}\\)分类为\\(c _i\\)所产生的期望损失(expected loss)，即咋样本\\(\vec{x}\\)上的条件风险(conditional risk)：
$$
R(c _i|\vec{x})=\sum _{j=1}^N\lambda _{ij}P(c _j|\vec{x})
$$
任务是寻找一个判别准则\\(h:\chi\mapsto y\\)以最小化总体风险
$$
R(h)=E _{\vec{x}}[R(h(\vec{x})|\vec{x})]
$$
* 贝叶斯判定准则(Bayes decision rule)：为最小化总体风险，只需要在每个样本上选择那个能使条件风险\\(R(c|\vec{x})\\)最小的类别标记，即
$$
h^*(\vec{x})=argmin _{c\in y}R(c|\vec{x})
$$
此时，\\(h^*\\)称为贝叶斯最优分类器(Bayes optimal classifier)，与之对应的总体风险\\(R(h^*)\\)称为贝叶斯风险(Bayes risk)。\\(1-R(h^*)\\)反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。若\\(\lambda _{ij}\\)为“0/1损失函数”，此时最小化分类错误率的贝叶斯最优分类器
$$
h^*(\vec{x})=argmax _{c\in y}P(c|\vec{x})
$$
即对每个样本\\(\vec{x}\\)，选择能使后验概率\\(P(c|\vec{x})\\)最大的类别标记。从这个角度看，机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率\\(P(c|\vec{x})\\)
* 判别式模型(discriminative models)：给定\\(\vec{x}\\)，可通过直接建模\\(P(c|\vec{x})\\)来预测*c*，例如决策树、BP神经网络、支持向量机等。生成式模型(generative models)：先对联合概率分布\\(P(\vec{x},c)\\)建模，然后再因此获得\\(P(c|\vec{x})\\)。对于生成式模型，有
$$
P(c|\vec{x})=\frac{P(\vec{x},c)}{P(\vec{x})}
$$
基于贝叶斯公式，有
$$
P(c|\vec{x})=\frac{P(c)P(\vec{x}|c)}{P(\vec{x})}
$$
其中\\(P(c)\\)是先验(prior)概率，\\(P(\vec{x}|c)\\)是样本\\(\vec{x}\\)相对于类标记*c*的类条件概率(class-conditional probability)，或称为似然(likelihood)，\\(P(\vec{x})\\)是用于归一化的证据(evidence)因子。
## 7.2 极大似然估计
* 假定\\(P(\vec{x}|c)\\)具有确定的形式并且被参数向量\\(\vec{\theta} _c\\)唯一确定，将\\(P(\vec{x}|c)\\)记为\\(P(\vec{x}|\vec{\theta} _c)\\)
* 概率模型的训练过程就是参数估计(parameter estimation)过程。频率主义学派(Frequentist)认为参数虽然未知，但却客观存在的固定值，因此可以通过优化似然函数等准则来确定参数值。贝叶斯学派(Bayesian)则认为参数是未观察到的随机变量，其本身也有分布，因此可以假定一个先验分布，然后基于观测到的数据来计算参数的后验分布
* MLE(Maximum Likelihood Estimation)是根据数据采样来估计概率分布参数的经典方法。令\\(D _c\\)表示训练集*D*中第*c*类样本组成的集合，假设这些样本是独立同分布的，则参数\\(\vec{\theta} _c\\)对于数据集\\(D _c\\)的似然是：
$$
P(D _c|\vec{\theta} _c)=\prod _{\vec{x}\in D _c}P(\vec{x}|\vec{\theta} _c)
$$
由于连乘操作容易造成下溢，通常使用对数似然(log-likelihood)
$$
\begin{aligned}
LL(\vec{\theta} _c)&=logP(D _c|\vec{\theta} _c) \\\\
&=\sum _{\vec{x}\in D _c}logP(\vec{x}|\vec{\theta} _c)
\end{aligned}
$$
此时参数\\(\vec{\theta} _c\\)的极大似然估计\\(\hat{\vec{\theta}} _c\\)为
$$
\hat{\vec{\theta}} _c=argmax _{\vec{\theta} _c}LL(\vec{\theta} _c)
$$
## 7.3 朴素贝叶斯分类器
* 朴素贝叶斯分类器(naive Bayes classifier)采用了属性条件独立性假设(attribute conditional independence assumption)：对已知类别，假设所有属性相互独立，则有
$$
P(c|\vec{x})=\frac{P(c)P(\vec{x}|c)}{P(\vec{x})}=\frac{P(c)}{P(\vec{x})}\prod _{i=1}^dP(x _i|c)
$$
类先验概率：
$$
P(c)=\frac{|D _c|}{|D|}
$$
对离散属性而言：
$$
P(x _i|c)=\frac{|D _{c,x _i}|}{D _c}
$$
对连续属性可考虑概率密度函数，假定\\(p(x _i|c)~N(\mu _{c,i},\sigma^2 _{c,i})\\)，有
$$
p(x _i|c)=\frac{1}{\sqrt{2\pi}\sigma _{c,i}}e^{-\frac{(x _i-\mu _{c,i})^2}{2\sigma^2 _{c,i}}}
$$
* 为了避免其他属性携带的信息被训练集中未出现的属性值抹去，在估计概率值时通常要进行平滑(smoothing)，常用拉普拉斯修正(Laplacian correction)。令*N*表示训练集*D*中可能的类别数，\\(N _i\\)表示第*i*个属性可能的取值数，则
$$
\begin{aligned}
\hat{P}(c)&=\frac{|D _c|+1}{|D|+N}\\\\
\hat{P}(x _i|c)&=\frac{|D _{c,x _i}|+1}{|D _c|+N _i}
\end{aligned}
$$
拉普拉斯修正实质上假设了属性值与类别均匀分布，这是在朴素贝叶斯学习过程中额外引入的关于数据的先验
## 7.4 半朴素贝叶斯分类器
* 半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于忽略掉比较强的属性依赖关系。
* 独依赖关系(One-Dependent Estimation)是半朴素贝叶斯分类器最常用的一种策略，即假设每个属性在类别之外最多仅依赖一个其他属性
$$
P(c|\vec{x})\propto P(c)\prod _{i=1}^dP(x _i|c, pa _i)
$$
其中\\(pa _i\\)是属性\\(x _i\\)所依赖的属性，称为\\(x _i\\)的父属性
* 最直接的做法是假设所有属性都依赖于同一个属性，称为超父(super-parent)，然后通过交叉验证法等模型选择方法来确定超父属性，由此形成SPODE(Super-Parent ODE)方法
* TAN(Tree Augmented naive Bayes)则是在最大带权生成树(maximum weighted spanning tree)算法的基础上，通过以下步骤将属性间依赖关系约减为树形结构：（1）计算任意两个属性之间的条件相互信息(conditional mutual information)
$$
I(x _i, x _j|y)=\sum _{x _i, x _j;c\in y}P(x _i,x _j|c)log\frac{P(x _i,x _j|c)}{P(x _i|c)P(x _j|c)}
$$；
（2）以属性为结点构建完全图，任意两个结点之间边的权重设为\\(I(x _i,x _j|y)\\)；（3）构建此完全图的最大带权生成树，挑选根变量，将边设置为有向；（4）加入类别结点*y*，增加从*y*到每个属性的有向边